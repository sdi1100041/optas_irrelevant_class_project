{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m/workspace/optas_irrelevant_class_project/main.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0mirr_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'irr_class'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0mregularization\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'regularization'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m     \u001b[0mconstruct_and_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/workspace/optas_irrelevant_class_project/main.py\u001b[0m in \u001b[0;36mconstruct_and_train\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mconstruct_and_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbest_acc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mground_truth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mglobal_net\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m     \u001b[0mtrainset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_train_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'task'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m     \u001b[0mtestset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_validation_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'task'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0mtrainloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/optas_irrelevant_class_project/utils/data_processing/get_dataset.py\u001b[0m in \u001b[0;36mget_train_data\u001b[0;34m(task)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#task == \"CIFAR100\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mdataset1\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCIFAR10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'../data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCIFAR10Transform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mAugmentedDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'../data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCIFAR10Transform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcifar10_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/optas_irrelevant_class_project/utils/data_processing/get_dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, train, download, transform, cifar10_dataset)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mcifar10_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcifar10_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcifar10_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcifar10_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "%run main.py --task=CIFAR100 --lr=0.001 --epochs=50 --algorithm=Adam --model=ResNet18 --regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "\n",
    "class CIFAR10Transform:\n",
    "    def train_transform():\n",
    "        return transforms.Compose([\n",
    "               transforms.RandomCrop(32, padding=4),\n",
    "               transforms.RandomHorizontalFlip(),\n",
    "               transforms.ToTensor(),\n",
    "               transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),])\n",
    "\n",
    "    def test_transform():\n",
    "        return transforms.Compose([\n",
    "               transforms.ToTensor(),\n",
    "               transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),])\n",
    "\n",
    "\n",
    "    \n",
    "dataset= datasets.CIFAR100(root='../data', train=True, download=True, transform=CIFAR10Transform.train_transform())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AugmentedDataset(datasets.cifar.CIFAR100):\n",
    "    def __init__(self,root, train, download, transform,cifar10_dataset):\n",
    "        super().__init__(root=root, train=train,download=download,transform=transform)\n",
    "        self.targets = np.array(self.targets)\n",
    "        indices = (self.targets != self.class_to_idx['bicycle']) & (self.targets != self.class_to_idx['bus']) & (self.targets != self.class_to_idx['motorcycle']) & (self.targets != self.class_to_idx['pickup_truck']) & (self.targets != self.class_to_idx['train']) & (self.targets != self.class_to_idx['lawn_mower']) & (self.targets != self.class_to_idx['streetcar']) & (self.targets != self.class_to_idx['rocket']) & (self.targets != self.class_to_idx['tank']) & (self.targets != self.class_to_idx['tractor'])\n",
    "        self.targets=self.targets[indices]\n",
    "        self.data = self.data[indices]\n",
    "        self.targets[:] = 10\n",
    "        self.improper_dataset_size = len(self.data)\n",
    "        self.improper_augmented_dataset_size=8*len(self.data)\n",
    "        self.proper_dataset_size=len(cifar10_dataset.data)\n",
    "        cifar10_dataset.targets=np.array(cifar10_dataset.targets)\n",
    "        self.targets = np.concatenate([self.targets, cifar10_dataset.targets])\n",
    "        self.data = np.concatenate([self.data,cifar10_dataset.data])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.improper_augmented_dataset_size + self.proper_dataset_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if (idx >= self.improper_augmented_dataset_size):\n",
    "            idx = self.improper_dataset_size + idx - self.improper_augmented_dataset_size\n",
    "            return self.data[idx], self.targets[idx]\n",
    "        \n",
    "        idx = idx % self.improper_dataset_size\n",
    "        trnsf= idx / self.improper_dataset_size\n",
    "        x = self.data[idx]\n",
    "        if (trnsf%2):\n",
    "            x=transforms.functional.vflip(x)\n",
    "        if ((trnsf/2)%2):\n",
    "            x=transforms.functional.hflip(x)\n",
    "        if ((trnsf/4)%2):\n",
    "            x=transforms.functional.rotate(x,90)\n",
    "        \n",
    "        return x, self.targets[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "dataset=AugmentedDataset(root='../data', train=True, download=True, transform=CIFAR10Transform.train_transform())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "360000"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = torch.utils.data.DataLoader(dataset, batch_size=100,\n",
    "                                          shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3600"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs=next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 70,  52,  35],\n",
       "          [113,  85,  66],\n",
       "          [105,  70,  52],\n",
       "          ...,\n",
       "          [ 41,  22,  17],\n",
       "          [ 46,  24,  19],\n",
       "          [ 32,  10,   6]],\n",
       "\n",
       "         [[ 77,  62,  43],\n",
       "          [ 80,  54,  41],\n",
       "          [108,  71,  45],\n",
       "          ...,\n",
       "          [ 41,  24,  19],\n",
       "          [ 40,  21,  17],\n",
       "          [ 25,   6,   3]],\n",
       "\n",
       "         [[ 55,  45,  25],\n",
       "          [ 72,  52,  36],\n",
       "          [145, 111,  74],\n",
       "          ...,\n",
       "          [ 31,  14,   9],\n",
       "          [ 34,  14,   9],\n",
       "          [ 27,   8,   4]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[179, 158, 135],\n",
       "          [169, 150, 125],\n",
       "          [181, 163, 138],\n",
       "          ...,\n",
       "          [206, 175, 154],\n",
       "          [194, 157, 140],\n",
       "          [151, 108,  95]],\n",
       "\n",
       "         [[183, 162, 145],\n",
       "          [178, 157, 132],\n",
       "          [184, 166, 137],\n",
       "          ...,\n",
       "          [209, 177, 152],\n",
       "          [165, 124, 104],\n",
       "          [106,  61,  48]],\n",
       "\n",
       "         [[194, 174, 156],\n",
       "          [174, 152, 125],\n",
       "          [158, 140, 110],\n",
       "          ...,\n",
       "          [188, 153, 127],\n",
       "          [129,  90,  71],\n",
       "          [ 74,  35,  26]]],\n",
       "\n",
       "\n",
       "        [[[255, 255, 255],\n",
       "          [255, 255, 255],\n",
       "          [255, 255, 255],\n",
       "          ...,\n",
       "          [255, 255, 255],\n",
       "          [255, 255, 255],\n",
       "          [255, 255, 255]],\n",
       "\n",
       "         [[255, 255, 255],\n",
       "          [254, 254, 254],\n",
       "          [254, 254, 254],\n",
       "          ...,\n",
       "          [254, 254, 254],\n",
       "          [254, 254, 254],\n",
       "          [254, 254, 254]],\n",
       "\n",
       "         [[255, 255, 255],\n",
       "          [254, 254, 254],\n",
       "          [255, 255, 255],\n",
       "          ...,\n",
       "          [255, 255, 255],\n",
       "          [255, 255, 255],\n",
       "          [255, 255, 255]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[255, 255, 255],\n",
       "          [254, 254, 254],\n",
       "          [255, 255, 255],\n",
       "          ...,\n",
       "          [255, 255, 255],\n",
       "          [255, 255, 255],\n",
       "          [255, 255, 255]],\n",
       "\n",
       "         [[255, 255, 255],\n",
       "          [254, 254, 254],\n",
       "          [255, 255, 255],\n",
       "          ...,\n",
       "          [255, 255, 255],\n",
       "          [255, 255, 255],\n",
       "          [255, 255, 255]],\n",
       "\n",
       "         [[255, 255, 255],\n",
       "          [254, 254, 254],\n",
       "          [255, 255, 255],\n",
       "          ...,\n",
       "          [255, 255, 255],\n",
       "          [255, 255, 255],\n",
       "          [255, 255, 255]]],\n",
       "\n",
       "\n",
       "        [[[152, 158, 158],\n",
       "          [180, 179, 183],\n",
       "          [ 49,  54,  50],\n",
       "          ...,\n",
       "          [137, 143, 116],\n",
       "          [164, 167, 129],\n",
       "          [160, 164, 123]],\n",
       "\n",
       "         [[163, 169, 169],\n",
       "          [172, 170, 171],\n",
       "          [ 52,  60,  59],\n",
       "          ...,\n",
       "          [140, 149, 124],\n",
       "          [138, 150, 122],\n",
       "          [125, 139, 106]],\n",
       "\n",
       "         [[171, 173, 173],\n",
       "          [164, 164, 162],\n",
       "          [ 60,  67,  63],\n",
       "          ...,\n",
       "          [147, 156, 131],\n",
       "          [152, 162, 134],\n",
       "          [148, 160, 129]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 87,  81,  72],\n",
       "          [ 93,  89,  81],\n",
       "          [ 95,  92,  83],\n",
       "          ...,\n",
       "          [134, 136, 142],\n",
       "          [142, 146, 153],\n",
       "          [148, 154, 161]],\n",
       "\n",
       "         [[ 84,  69,  60],\n",
       "          [ 84,  73,  65],\n",
       "          [ 81,  70,  62],\n",
       "          ...,\n",
       "          [140, 142, 146],\n",
       "          [142, 147, 156],\n",
       "          [144, 151, 160]],\n",
       "\n",
       "         [[ 73,  56,  45],\n",
       "          [ 81,  63,  51],\n",
       "          [ 86,  65,  52],\n",
       "          ...,\n",
       "          [131, 131, 136],\n",
       "          [125, 128, 134],\n",
       "          [128, 132, 139]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[226, 226, 226],\n",
       "          [221, 221, 221],\n",
       "          [220, 220, 221],\n",
       "          ...,\n",
       "          [ 62,  57,  54],\n",
       "          [ 55,  50,  47],\n",
       "          [ 40,  36,  31]],\n",
       "\n",
       "         [[226, 226, 226],\n",
       "          [225, 225, 225],\n",
       "          [226, 226, 226],\n",
       "          ...,\n",
       "          [ 60,  55,  50],\n",
       "          [ 54,  49,  44],\n",
       "          [ 38,  34,  27]],\n",
       "\n",
       "         [[226, 226, 226],\n",
       "          [229, 229, 229],\n",
       "          [232, 232, 232],\n",
       "          ...,\n",
       "          [ 55,  51,  43],\n",
       "          [ 50,  46,  38],\n",
       "          [ 44,  40,  31]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 25,  22,  25],\n",
       "          [ 60,  62,  72],\n",
       "          [115, 122, 139],\n",
       "          ...,\n",
       "          [124, 122, 115],\n",
       "          [126, 126, 127],\n",
       "          [116, 116, 126]],\n",
       "\n",
       "         [[ 72,  73,  85],\n",
       "          [121, 124, 141],\n",
       "          [145, 152, 174],\n",
       "          ...,\n",
       "          [129, 127, 124],\n",
       "          [114, 114, 118],\n",
       "          [101, 102, 112]],\n",
       "\n",
       "         [[119, 128, 147],\n",
       "          [142, 149, 169],\n",
       "          [149, 154, 176],\n",
       "          ...,\n",
       "          [111, 112, 114],\n",
       "          [102, 103, 109],\n",
       "          [103, 103, 112]]],\n",
       "\n",
       "\n",
       "        [[[  8, 104, 158],\n",
       "          [  2, 103, 159],\n",
       "          [  0, 104, 159],\n",
       "          ...,\n",
       "          [  1, 120, 172],\n",
       "          [  1, 120, 171],\n",
       "          [  1, 120, 171]],\n",
       "\n",
       "         [[  8, 109, 163],\n",
       "          [  2, 108, 165],\n",
       "          [  1, 110, 166],\n",
       "          ...,\n",
       "          [  1, 126, 177],\n",
       "          [  1, 125, 177],\n",
       "          [  1, 125, 176]],\n",
       "\n",
       "         [[  7, 113, 166],\n",
       "          [  1, 112, 168],\n",
       "          [  0, 113, 168],\n",
       "          ...,\n",
       "          [  1, 130, 182],\n",
       "          [  2, 131, 182],\n",
       "          [  1, 130, 182]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[129, 109,  94],\n",
       "          [ 99,  79,  67],\n",
       "          [ 57,  39,  31],\n",
       "          ...,\n",
       "          [195, 154, 106],\n",
       "          [194, 153, 105],\n",
       "          [189, 151, 102]],\n",
       "\n",
       "         [[126, 102,  86],\n",
       "          [122,  98,  82],\n",
       "          [ 65,  45,  36],\n",
       "          ...,\n",
       "          [166, 135, 100],\n",
       "          [183, 149, 108],\n",
       "          [187, 150, 105]],\n",
       "\n",
       "         [[118, 100,  82],\n",
       "          [121, 103,  83],\n",
       "          [ 89,  78,  63],\n",
       "          ...,\n",
       "          [ 74,  54,  42],\n",
       "          [146, 117,  95],\n",
       "          [181, 144, 112]]],\n",
       "\n",
       "\n",
       "        [[[  1,   2,   2],\n",
       "          [  8,  13,  10],\n",
       "          [ 22,  35,  29],\n",
       "          ...,\n",
       "          [ 31,  32,  27],\n",
       "          [  8,   8,   6],\n",
       "          [  2,   2,   1]],\n",
       "\n",
       "         [[  1,   1,   2],\n",
       "          [  0,   2,   1],\n",
       "          [  4,  12,  12],\n",
       "          ...,\n",
       "          [ 35,  36,  31],\n",
       "          [ 24,  25,  21],\n",
       "          [  5,   5,   5]],\n",
       "\n",
       "         [[  1,   0,   1],\n",
       "          [  1,   2,   2],\n",
       "          [  4,  12,  12],\n",
       "          ...,\n",
       "          [ 34,  34,  29],\n",
       "          [ 29,  30,  25],\n",
       "          [ 21,  21,  21]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[221, 232, 211],\n",
       "          [217, 230, 228],\n",
       "          [220, 229, 201],\n",
       "          ...,\n",
       "          [ 59,  54,  65],\n",
       "          [ 64,  60,  75],\n",
       "          [ 56,  51,  67]],\n",
       "\n",
       "         [[227, 240, 245],\n",
       "          [223, 234, 234],\n",
       "          [227, 235, 230],\n",
       "          ...,\n",
       "          [ 87,  87, 106],\n",
       "          [ 61,  61,  73],\n",
       "          [ 58,  53,  58]],\n",
       "\n",
       "         [[225, 238, 235],\n",
       "          [232, 242, 240],\n",
       "          [206, 210, 198],\n",
       "          ...,\n",
       "          [ 98, 100, 121],\n",
       "          [ 86,  88, 110],\n",
       "          [ 69,  64,  77]]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_img=dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_img_flip= transforms.functional.vflip(my_img[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_img_rot=transforms.functional.rotate(my_img[0],90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "imshow(my_img[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Epoch: 0\n",
      " [================================================================>]  Step: 51ms | Tot: 19s2ms | Loss: 1.848 | Acc: 26.742% (13371/50000) 391/391 1 ......]  Step: 51ms | Tot: 2s180ms | Loss: 2.186 | Acc: 16.644% (980/5888) 46/391 .....................]  Step: 49ms | Tot: 2s230ms | Loss: 2.179 | Acc: 16.722% (1006/6016) 47/391  144/391 ]  Step: 50ms | Tot: 7s564ms | Loss: 2.006 | Acc: 20.106% (3989/19840) 155/391 =========================>.....................................]  Step: 45ms | Tot: 8s62ms | Loss: 1.996 | Acc: 20.507% (4331/21120) 165/391  172/391 ===================================>.............................]  Step: 43ms | Tot: 10s579ms | Loss: 1.957 | Acc: 22.320% (6171/27648) 216/391  283/391   Step: 63ms | Tot: 14s433ms | Loss: 1.915 | Acc: 23.895% (9084/38016) 297/391 ......]  Step: 57ms | Tot: 17s157ms | Loss: 1.876 | Acc: 25.557% (11515/45056) 352/391 ..]  Step: 68ms | Tot: 18s209ms | Loss: 1.860 | Acc: 26.279% (12614/48000) 375/391  378/391 \n",
      " [================================================================>]  Step: 19ms | Tot: 1s896ms | Loss: 1.753 | Acc: 35.080% (3508/10000) 100/100 \n",
      "Saving..\n",
      "\n",
      "Epoch: 1\n",
      " [================================================================>]  Step: 45ms | Tot: 18s620ms | Loss: 1.332 | Acc: 49.614% (24807/50000) 391/391 =====>.......................................................]  Step: 41ms | Tot: 2s566ms | Loss: 1.538 | Acc: 39.160% (2807/7168) 56/391 ===================>.....................................]  Step: 52ms | Tot: 7s897ms | Loss: 1.463 | Acc: 43.205% (9125/21120) 165/391 .]  Step: 58ms | Tot: 9s640ms | Loss: 1.441 | Acc: 44.338% (11464/25856) 202/391 ==================================>..............................]  Step: 52ms | Tot: 9s896ms | Loss: 1.437 | Acc: 44.531% (11799/26496) 207/391 ===========================>.......]  Step: 48ms | Tot: 16s492ms | Loss: 1.358 | Acc: 48.419% (21506/44416) 347/391 =======================================>.....]  Step: 45ms | Tot: 17s143ms | Loss: 1.350 | Acc: 48.761% (22469/46080) 360/391 \n",
      " [================================================================>]  Step: 21ms | Tot: 1s928ms | Loss: 1.407 | Acc: 53.150% (5315/10000) 100/100 =======================>.....................................]  Step: 17ms | Tot: 813ms | Loss: 1.418 | Acc: 53.884% (2317/4300) 43/100 ===================================================>.........]  Step: 18ms | Tot: 1s602ms | Loss: 1.414 | Acc: 53.221% (4577/8600) 86/100  88/100 \n",
      "Test error is 0.4685 0.4685\n",
      "Hamming distance between rounds is 0.5779\n",
      "Test error of previous epoch is 0.6492\n",
      "Saving..\n",
      "\n",
      "Epoch: 2\n",
      " [================================================================>]  Step: 49ms | Tot: 18s778ms | Loss: 0.989 | Acc: 64.240% (32120/50000) 391/391 .........................................]  Step: 65ms | Tot: 2s405ms | Loss: 1.067 | Acc: 60.500% (3872/6400) 50/391 =========>.......................................................]  Step: 45ms | Tot: 2s806ms | Loss: 1.063 | Acc: 60.818% (4593/7552) 59/391 .................................]  Step: 57ms | Tot: 7s88ms | Loss: 1.052 | Acc: 61.360% (11624/18944) 148/391 ==================================>..............................]  Step: 54ms | Tot: 10s47ms | Loss: 1.028 | Acc: 62.489% (16717/26752) 209/391 ...]  Step: 54ms | Tot: 11s171ms | Loss: 1.024 | Acc: 62.652% (18605/29696) 232/391  352/391 \n",
      " [================================================================>]  Step: 25ms | Tot: 2s210ms | Loss: 1.301 | Acc: 59.110% (5911/10000) 100/100 ]  Step: 26ms | Tot: 1s311ms | Loss: 1.328 | Acc: 58.559% (3455/5900) 59/100   Step: 24ms | Tot: 1s598ms | Loss: 1.328 | Acc: 58.704% (4168/7100) 71/100 .]  Step: 26ms | Tot: 1s675ms | Loss: 1.329 | Acc: 58.662% (4341/7400) 74/100 ]  Step: 24ms | Tot: 1s700ms | Loss: 1.327 | Acc: 58.707% (4403/7500) 75/100 ==============================================>................]  Step: 24ms | Tot: 1s725ms | Loss: 1.324 | Acc: 58.737% (4464/7600) 76/100  78/100 \n",
      "Test error is 0.4089 0.40890000000000004\n",
      "Hamming distance between rounds is 0.3635\n",
      "Test error of previous epoch is 0.4685\n",
      "Saving..\n",
      "\n",
      "Epoch: 3\n",
      " [================================================================>]  Step: 43ms | Tot: 18s552ms | Loss: 0.825 | Acc: 71.204% (35602/50000) 391/391 ==========>..................................................]  Step: 42ms | Tot: 4s237ms | Loss: 0.867 | Acc: 69.609% (8108/11648) 91/391 =========================================>..................]  Step: 57ms | Tot: 13s192ms | Loss: 0.848 | Acc: 70.425% (25150/35712) 279/391 \n",
      " [================================================================>]  Step: 15ms | Tot: 2s19ms | Loss: 0.815 | Acc: 72.010% (7201/10000) 100/100 Step: 23ms | Tot: 324ms | Loss: 0.787 | Acc: 72.375% (1158/1600) 16/100 .]  Step: 25ms | Tot: 399ms | Loss: 0.788 | Acc: 72.105% (1370/1900) 19/100 =============================================================>...]  Step: 19ms | Tot: 1s948ms | Loss: 0.815 | Acc: 72.031% (6915/9600) 96/100 \n",
      "Test error is 0.2799 0.2798999999999999\n",
      "Hamming distance between rounds is 0.3274\n",
      "Test error of previous epoch is 0.4089\n",
      "Saving..\n",
      "\n",
      "Epoch: 4\n",
      " [================================================================>]  Step: 57ms | Tot: 18s677ms | Loss: 0.710 | Acc: 75.767% (37823/49920) 390/391 ep: 62ms | Tot: 2s596ms | Loss: 0.745 | Acc: 74.062% (5214/7040) 55/391 ..............]  Step: 49ms | Tot: 13s702ms | Loss: 0.724 | Acc: 75.289% (27851/36992) 289/391  290/391 .]  Step: 49ms | Tot: 17s890ms | Loss: 0.712 | Acc: 75.662% (36124/47744) 373/391 ================================================================>]  Step: 52ms | Tot: 18s730ms | Loss: 0.710 | Acc: 75.760% (37880/50000) 391/391 \n",
      " [================================================================>]  Step: 19ms | Tot: 1s893ms | Loss: 0.714 | Acc: 75.770% (7577/10000) 100/100 ...........................]  Step: 19ms | Tot: 56ms | Loss: 0.648 | Acc: 76.250% (305/400) 4/100 ]  Step: 19ms | Tot: 745ms | Loss: 0.701 | Acc: 76.000% (3040/4000) 40/100 \n",
      "Test error is 0.2423 0.24230000000000007\n",
      "Hamming distance between rounds is 0.1827\n",
      "Test error of previous epoch is 0.2799\n",
      "Saving..\n",
      "\n",
      "Epoch: 5\n",
      " [================================================================>]  Step: 36ms | Tot: 18s666ms | Loss: 0.616 | Acc: 79.326% (39663/50000) 391/391 tep: 64ms | Tot: 1s338ms | Loss: 0.616 | Acc: 79.635% (3058/3840) 30/391 .......]  Step: 63ms | Tot: 10s183ms | Loss: 0.631 | Acc: 78.836% (21292/27008) 211/391 ........]  Step: 68ms | Tot: 13s26ms | Loss: 0.628 | Acc: 78.912% (27373/34688) 271/391 .......]  Step: 51ms | Tot: 15s127ms | Loss: 0.623 | Acc: 79.149% (32014/40448) 316/391 \n",
      " [================================================================>]  Step: 16ms | Tot: 1s941ms | Loss: 0.674 | Acc: 77.970% (7797/10000) 100/100 ep: 21ms | Tot: 242ms | Loss: 0.651 | Acc: 79.250% (951/1200) 12/100 \n",
      "Test error is 0.2203 0.22030000000000005\n",
      "Hamming distance between rounds is 0.1875\n",
      "Test error of previous epoch is 0.2423\n",
      "Saving..\n",
      "\n",
      "Epoch: 6\n",
      " [================================================================>]  Step: 44ms | Tot: 18s854ms | Loss: 0.556 | Acc: 81.454% (40727/50000) 391/391 ....................]  Step: 58ms | Tot: 5s511ms | Loss: 0.581 | Acc: 80.489% (11642/14464) 113/391 ....................]  Step: 48ms | Tot: 6s763ms | Loss: 0.576 | Acc: 80.636% (14450/17920) 140/391 \n",
      " [================================================================>]  Step: 27ms | Tot: 1s982ms | Loss: 0.672 | Acc: 78.130% (7813/10000) 100/100 100 ===================>..........]  Step: 22ms | Tot: 1s670ms | Loss: 0.677 | Acc: 77.929% (6624/8500) 85/100 ====>..]  Step: 22ms | Tot: 1s915ms | Loss: 0.668 | Acc: 78.278% (7593/9700) 97/100 \n",
      "Test error is 0.2187 0.2187\n",
      "Hamming distance between rounds is 0.1786\n",
      "Test error of previous epoch is 0.2203\n",
      "Saving..\n",
      "\n",
      "Epoch: 7\n",
      " [================================================================>]  Step: 41ms | Tot: 18s814ms | Loss: 0.499 | Acc: 83.426% (41713/50000) 391/391 .]  Step: 57ms | Tot: 10s981ms | Loss: 0.509 | Acc: 83.172% (23847/28672) 224/391 ....]  Step: 61ms | Tot: 13s999ms | Loss: 0.506 | Acc: 83.221% (30785/36992) 289/391 .]  Step: 63ms | Tot: 15s767ms | Loss: 0.504 | Acc: 83.278% (34857/41856) 327/391 .....]  Step: 63ms | Tot: 16s278ms | Loss: 0.503 | Acc: 83.311% (35937/43136) 337/391 \n",
      " [================================================================>]  Step: 20ms | Tot: 2s63ms | Loss: 0.575 | Acc: 81.520% (8152/10000) 100/100 /100 =======>............................................]  Step: 20ms | Tot: 688ms | Loss: 0.569 | Acc: 81.781% (2617/3200) 32/100 ......]  Step: 17ms | Tot: 948ms | Loss: 0.573 | Acc: 81.867% (3684/4500) 45/100   Step: 21ms | Tot: 1s612ms | Loss: 0.580 | Acc: 81.403% (6268/7700) 77/100 ==========================================================>......]  Step: 20ms | Tot: 1s893ms | Loss: 0.583 | Acc: 81.286% (7397/9100) 91/100 \n",
      "Test error is 0.1848 0.18480000000000008\n",
      "Hamming distance between rounds is 0.2014\n",
      "Test error of previous epoch is 0.2187\n",
      "Saving..\n",
      "\n",
      "Epoch: 8\n",
      " [================================================================>]  Step: 42ms | Tot: 18s927ms | Loss: 0.451 | Acc: 85.000% (42500/50000) 391/391 ============================>.................................]  Step: 50ms | Tot: 9s210ms | Loss: 0.460 | Acc: 84.712% (20602/24320) 190/391 ...]  Step: 60ms | Tot: 14s732ms | Loss: 0.456 | Acc: 84.849% (33125/39040) 305/391 ================================================================>]  Step: 50ms | Tot: 18s833ms | Loss: 0.451 | Acc: 84.990% (42318/49792) 389/391 \n",
      " [================================================================>]  Step: 17ms | Tot: 1s890ms | Loss: 0.557 | Acc: 83.170% (8317/10000) 100/100 \n",
      "Test error is 0.1683 0.1683\n",
      "Hamming distance between rounds is 0.1482\n",
      "Test error of previous epoch is 0.1848\n",
      "Saving..\n",
      "\n",
      "Epoch: 9\n",
      " [================================================================>]  Step: 43ms | Tot: 18s851ms | Loss: 0.417 | Acc: 86.216% (43108/50000) 391/391 .................................]  Step: 65ms | Tot: 2s67ms | Loss: 0.413 | Acc: 86.186% (4854/5632) 44/391 .......]  Step: 59ms | Tot: 6s769ms | Loss: 0.420 | Acc: 86.059% (15532/18048) 141/391 ======================================>.........................]  Step: 50ms | Tot: 11s538ms | Loss: 0.419 | Acc: 86.151% (26576/30848) 241/391 ========>...............]  Step: 52ms | Tot: 14s184ms | Loss: 0.422 | Acc: 86.066% (32719/38016) 297/391 =======================================>.......]  Step: 46ms | Tot: 16s576ms | Loss: 0.419 | Acc: 86.157% (38157/44288) 346/391 \n",
      " [================================================================>]  Step: 23ms | Tot: 2s186ms | Loss: 0.700 | Acc: 78.830% (7883/10000) 100/100 ...............]  Step: 21ms | Tot: 40ms | Loss: 0.654 | Acc: 77.667% (233/300) 3/100 \n",
      "Test error is 0.2117 0.2117\n",
      "Hamming distance between rounds is 0.1883\n",
      "Test error of previous epoch is 0.1683\n",
      "\n",
      "Epoch: 10\n",
      " [================================================================>]  Step: 37ms | Tot: 18s908ms | Loss: 0.380 | Acc: 87.396% (43698/50000) 391/391 ...........]  Step: 59ms | Tot: 407ms | Loss: 0.391 | Acc: 86.719% (999/1152) 9/391 =====>...........................................................]  Step: 54ms | Tot: 1s675ms | Loss: 0.380 | Acc: 87.316% (3800/4352) 34/391  46/391 .................]  Step: 50ms | Tot: 3s766ms | Loss: 0.385 | Acc: 87.043% (8579/9856) 77/391 ===================>.............................................]  Step: 41ms | Tot: 5s692ms | Loss: 0.388 | Acc: 86.954% (12911/14848) 116/391 ...]  Step: 57ms | Tot: 6s | Loss: 0.389 | Acc: 86.975% (13582/15616) 122/391 ..]  Step: 62ms | Tot: 10s565ms | Loss: 0.387 | Acc: 87.174% (24325/27904) 218/391 .]  Step: 58ms | Tot: 14s645ms | Loss: 0.385 | Acc: 87.218% (33715/38656) 302/391 \n",
      " [================================================================>]  Step: 15ms | Tot: 2s36ms | Loss: 0.524 | Acc: 83.970% (8397/10000) 100/100 \n",
      "Test error is 0.1603 0.1603\n",
      "Hamming distance between rounds is 0.1763\n",
      "Test error of previous epoch is 0.2117\n",
      "Saving..\n",
      "\n",
      "Epoch: 11\n",
      " [================================================================>]  Step: 43ms | Tot: 18s776ms | Loss: 0.349 | Acc: 88.414% (44207/50000) 391/391 ......................................................]  Step: 62ms | Tot: 300ms | Loss: 0.360 | Acc: 88.170% (790/896) 7/391 ......]  Step: 61ms | Tot: 3s388ms | Loss: 0.348 | Acc: 88.123% (7783/8832) 69/391 ====================>............................................]  Step: 53ms | Tot: 5s968ms | Loss: 0.352 | Acc: 88.225% (14116/16000) 125/391 =================================>...............................]  Step: 42ms | Tot: 9s751ms | Loss: 0.353 | Acc: 88.227% (22586/25600) 200/391 =======================================>.........................]  Step: 55ms | Tot: 11s442ms | Loss: 0.348 | Acc: 88.424% (26711/30208) 236/391 ..]  Step: 54ms | Tot: 13s552ms | Loss: 0.353 | Acc: 88.320% (31654/35840) 280/391 ======================================================>......]  Step: 54ms | Tot: 16s904ms | Loss: 0.350 | Acc: 88.392% (39939/45184) 353/391 \n",
      " [================================================================>]  Step: 19ms | Tot: 1s929ms | Loss: 0.498 | Acc: 84.800% (8480/10000) 100/100 ...]  Step: 24ms | Tot: 42ms | Loss: 0.515 | Acc: 82.000% (246/300) 3/100 ...]  Step: 24ms | Tot: 261ms | Loss: 0.495 | Acc: 84.000% (1260/1500) 15/100   Step: 20ms | Tot: 1s385ms | Loss: 0.497 | Acc: 84.658% (6434/7600) 76/100  84/100 .]  Step: 24ms | Tot: 1s661ms | Loss: 0.500 | Acc: 84.713% (7370/8700) 87/100 \n",
      "Test error is 0.152 0.15200000000000002\n",
      "Hamming distance between rounds is 0.1246\n",
      "Test error of previous epoch is 0.1603\n",
      "Saving..\n",
      "\n",
      "Epoch: 12\n",
      " [================================================================>]  Step: 44ms | Tot: 19s54ms | Loss: 0.326 | Acc: 89.108% (44554/50000) 391/391  ]  Step: 59ms | Tot: 4s451ms | Loss: 0.322 | Acc: 89.300% (10173/11392) 89/391 ===================>.............................................]  Step: 42ms | Tot: 5s762ms | Loss: 0.326 | Acc: 89.150% (13237/14848) 116/391 ]  Step: 54ms | Tot: 8s338ms | Loss: 0.325 | Acc: 89.222% (19072/21376) 167/391 ==============================>..................................]  Step: 48ms | Tot: 9s195ms | Loss: 0.326 | Acc: 89.172% (21116/23680) 185/391 ...]  Step: 43ms | Tot: 9s604ms | Loss: 0.325 | Acc: 89.152% (22024/24704) 193/391 =========================================>.......................]  Step: 48ms | Tot: 12s301ms | Loss: 0.325 | Acc: 89.153% (28529/32000) 250/391 ......................]  Step: 55ms | Tot: 12s454ms | Loss: 0.325 | Acc: 89.167% (28876/32384) 253/391 \n",
      " [================================================================>]  Step: 19ms | Tot: 2s126ms | Loss: 0.474 | Acc: 85.690% (8569/10000) 100/100 ....................]  Step: 20ms | Tot: 514ms | Loss: 0.470 | Acc: 85.636% (1884/2200) 22/100 =========>..........................................]  Step: 27ms | Tot: 800ms | Loss: 0.489 | Acc: 85.500% (3078/3600) 36/100  39/100 =======================================>.........................]  Step: 18ms | Tot: 1s351ms | Loss: 0.480 | Acc: 85.754% (5231/6100) 61/100   Step: 28ms | Tot: 1s870ms | Loss: 0.478 | Acc: 85.586% (7446/8700) 87/100 \n",
      "Test error is 0.1431 0.1431\n",
      "Hamming distance between rounds is 0.13\n",
      "Test error of previous epoch is 0.152\n",
      "Saving..\n",
      "\n",
      "Epoch: 13\n",
      " [================================================================>]  Step: 48ms | Tot: 18s739ms | Loss: 0.302 | Acc: 89.936% (44968/50000) 391/391 ep: 61ms | Tot: 1s61ms | Loss: 0.290 | Acc: 90.115% (2653/2944) 23/391 ..............................]  Step: 62ms | Tot: 5s169ms | Loss: 0.305 | Acc: 89.712% (12287/13696) 107/391 ............]  Step: 68ms | Tot: 6s438ms | Loss: 0.308 | Acc: 89.748% (15049/16768) 131/391 =======>...........]  Step: 50ms | Tot: 15s430ms | Loss: 0.306 | Acc: 89.824% (36792/40960) 320/391 \n",
      " [================================================================>]  Step: 17ms | Tot: 1s868ms | Loss: 0.487 | Acc: 85.180% (8518/10000) 100/100 \n",
      "Test error is 0.1482 0.1481999999999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming distance between rounds is 0.1282\n",
      "Test error of previous epoch is 0.1431\n",
      "\n",
      "Epoch: 14\n",
      " [================================================================>]  Step: 40ms | Tot: 19s73ms | Loss: 0.285 | Acc: 90.560% (45280/50000) 391/391  ...........................................................]  Step: 50ms | Tot: 896ms | Loss: 0.295 | Acc: 90.337% (2197/2432) 19/391 =========>.....................................................]  Step: 50ms | Tot: 3s468ms | Loss: 0.289 | Acc: 90.569% (8115/8960) 70/391 ==================>............................................]  Step: 53ms | Tot: 6s346ms | Loss: 0.293 | Acc: 90.489% (14594/16128) 126/391 ................]  Step: 60ms | Tot: 8s662ms | Loss: 0.290 | Acc: 90.495% (20155/22272) 174/391 \n",
      " [================================================================>]  Step: 15ms | Tot: 2s22ms | Loss: 0.479 | Acc: 86.460% (8646/10000) 100/100 ...........]  Step: 26ms | Tot: 1s203ms | Loss: 0.487 | Acc: 86.491% (4930/5700) 57/100 \n",
      "Test error is 0.1354 0.13540000000000008\n",
      "Hamming distance between rounds is 0.1328\n",
      "Test error of previous epoch is 0.1482\n",
      "Saving..\n",
      "\n",
      "Epoch: 15\n",
      " [================================================================>]  Step: 51ms | Tot: 18s865ms | Loss: 0.267 | Acc: 91.184% (45592/50000) 391/391 .]  Step: 53ms | Tot: 3s895ms | Loss: 0.257 | Acc: 91.248% (9227/10112) 79/391 ....]  Step: 66ms | Tot: 5s118ms | Loss: 0.265 | Acc: 90.990% (12229/13440) 105/391 ..]  Step: 59ms | Tot: 14s729ms | Loss: 0.272 | Acc: 91.018% (35417/38912) 304/391 ========================================================>........]  Step: 57ms | Tot: 16s530ms | Loss: 0.269 | Acc: 91.112% (39885/43776) 342/391 ==============================================================>..]  Step: 49ms | Tot: 18s222ms | Loss: 0.269 | Acc: 91.126% (43974/48256) 377/391 \n",
      " [================================================================>]  Step: 17ms | Tot: 1s897ms | Loss: 0.468 | Acc: 86.150% (8615/10000) 100/100 \n",
      "Test error is 0.1385 0.13849999999999996\n",
      "Hamming distance between rounds is 0.1298\n",
      "Test error of previous epoch is 0.1354\n",
      "\n",
      "Epoch: 16\n",
      " [================================================================>]  Step: 41ms | Tot: 18s930ms | Loss: 0.247 | Acc: 91.868% (45934/50000) 391/391 ........................................................]  Step: 46ms | Tot: 256ms | Loss: 0.214 | Acc: 92.839% (713/768) 6/391  23/391 ................]  Step: 58ms | Tot: 2s626ms | Loss: 0.236 | Acc: 92.159% (6488/7040) 55/391 ..]  Step: 61ms | Tot: 5s14ms | Loss: 0.244 | Acc: 91.970% (12243/13312) 104/391 .............]  Step: 59ms | Tot: 9s814ms | Loss: 0.248 | Acc: 91.793% (23969/26112) 204/391 ....]  Step: 59ms | Tot: 17s389ms | Loss: 0.246 | Acc: 91.872% (42217/45952) 359/391 ==============================================================>..]  Step: 53ms | Tot: 18s287ms | Loss: 0.247 | Acc: 91.852% (44324/48256) 377/391 \n",
      " [================================================================>]  Step: 20ms | Tot: 2s3ms | Loss: 0.453 | Acc: 87.280% (8728/10000) 100/100 \n",
      "Test error is 0.1272 0.12719999999999998\n",
      "Hamming distance between rounds is 0.1142\n",
      "Test error of previous epoch is 0.1385\n",
      "Saving..\n",
      "\n",
      "Epoch: 17\n",
      " [================================================================>]  Step: 45ms | Tot: 18s816ms | Loss: 0.231 | Acc: 92.406% (46203/50000) 391/391 ...............................................]  Step: 53ms | Tot: 202ms | Loss: 0.188 | Acc: 93.125% (596/640) 5/391 ====>..........................................................]  Step: 50ms | Tot: 1s907ms | Loss: 0.227 | Acc: 92.852% (4754/5120) 40/391 ..]  Step: 62ms | Tot: 11s397ms | Loss: 0.234 | Acc: 92.394% (27792/30080) 235/391 ============================================================>....]  Step: 43ms | Tot: 17s522ms | Loss: 0.234 | Acc: 92.370% (42919/46464) 363/391 =============================================================>...]  Step: 51ms | Tot: 17s984ms | Loss: 0.233 | Acc: 92.389% (44110/47744) 373/391 \n",
      " [================================================================>]  Step: 20ms | Tot: 1s997ms | Loss: 0.435 | Acc: 87.470% (8747/10000) 100/100 100 ====================>.............]  Step: 22ms | Tot: 1s595ms | Loss: 0.436 | Acc: 87.513% (7001/8000) 80/100 \n",
      "Test error is 0.1253 0.12529999999999997\n",
      "Hamming distance between rounds is 0.1072\n",
      "Test error of previous epoch is 0.1272\n",
      "Saving..\n",
      "\n",
      "Epoch: 18\n",
      " [================================================================>]  Step: 56ms | Tot: 19s175ms | Loss: 0.221 | Acc: 92.622% (46311/50000) 391/391 ..................]  Step: 57ms | Tot: 2s900ms | Loss: 0.227 | Acc: 92.418% (7216/7808) 61/391 ........]  Step: 53ms | Tot: 11s973ms | Loss: 0.226 | Acc: 92.485% (29240/31616) 247/391 \n",
      " [================================================================>]  Step: 17ms | Tot: 1s962ms | Loss: 0.419 | Acc: 87.790% (8779/10000) 100/100 ..................................]  Step: 22ms | Tot: 654ms | Loss: 0.413 | Acc: 88.156% (2821/3200) 32/100 ========================>........................................]  Step: 18ms | Tot: 772ms | Loss: 0.424 | Acc: 87.789% (3336/3800) 38/100 \n",
      "Test error is 0.1221 0.12209999999999999\n",
      "Hamming distance between rounds is 0.1117\n",
      "Test error of previous epoch is 0.1253\n",
      "Saving..\n",
      "\n",
      "Epoch: 19\n",
      " [================================================================>]  Step: 50ms | Tot: 18s709ms | Loss: 0.206 | Acc: 93.192% (46596/50000) 391/391 ............................................]  Step: 41ms | Tot: 4s629ms | Loss: 0.199 | Acc: 93.347% (11590/12416) 97/391 ..]  Step: 62ms | Tot: 5s395ms | Loss: 0.199 | Acc: 93.366% (13624/14592) 114/391 ......]  Step: 51ms | Tot: 12s624ms | Loss: 0.207 | Acc: 93.123% (31468/33792) 264/391 =============================================>...................]  Step: 43ms | Tot: 13s158ms | Loss: 0.208 | Acc: 93.077% (32644/35072) 274/391 \n",
      " [================================================================>]  Step: 21ms | Tot: 1s888ms | Loss: 0.547 | Acc: 85.340% (8534/10000) 100/100 .....]  Step: 22ms | Tot: 1s114ms | Loss: 0.550 | Acc: 85.414% (4954/5800) 58/100 ]  Step: 24ms | Tot: 1s350ms | Loss: 0.545 | Acc: 85.535% (6073/7100) 71/100 \n",
      "Test error is 0.1466 0.14659999999999995\n",
      "Hamming distance between rounds is 0.131\n",
      "Test error of previous epoch is 0.1221\n",
      "\n",
      "Epoch: 20\n",
      " [================================================================>]  Step: 41ms | Tot: 18s833ms | Loss: 0.193 | Acc: 93.582% (46791/50000) 391/391 .]  Step: 51ms | Tot: 6s360ms | Loss: 0.189 | Acc: 93.633% (16060/17152) 134/391 ....]  Step: 57ms | Tot: 11s801ms | Loss: 0.191 | Acc: 93.478% (28836/30848) 241/391 ==============>............]  Step: 46ms | Tot: 15s153ms | Loss: 0.195 | Acc: 93.446% (37558/40192) 314/391 \n",
      " [================================================================>]  Step: 17ms | Tot: 1s920ms | Loss: 0.496 | Acc: 87.080% (8708/10000) 100/100 ...............................]  Step: 20ms | Tot: 44ms | Loss: 0.370 | Acc: 88.667% (266/300) 3/100 ======================>.]  Step: 18ms | Tot: 1s884ms | Loss: 0.496 | Acc: 87.061% (8532/9800) 98/100 \n",
      "Test error is 0.1292 0.12919999999999998\n",
      "Hamming distance between rounds is 0.1346\n",
      "Test error of previous epoch is 0.1466\n",
      "\n",
      "Epoch: 21\n",
      " [================================================================>]  Step: 41ms | Tot: 18s829ms | Loss: 0.182 | Acc: 93.906% (46953/50000) 391/391 177/391 =====>........]  Step: 59ms | Tot: 16s500ms | Loss: 0.183 | Acc: 93.908% (40989/43648) 341/391 ===============================================================>.]  Step: 55ms | Tot: 18s518ms | Loss: 0.182 | Acc: 93.905% (46156/49152) 384/391 \n",
      " [================================================================>]  Step: 16ms | Tot: 2s67ms | Loss: 0.510 | Acc: 85.900% (8590/10000) 100/100 tep: 21ms | Tot: 264ms | Loss: 0.450 | Acc: 87.429% (1224/1400) 14/100 \n",
      "Test error is 0.141 0.1409999999999999\n",
      "Hamming distance between rounds is 0.1316\n",
      "Test error of previous epoch is 0.1292\n",
      "\n",
      "Epoch: 22\n",
      " [================================================================>]  Step: 46ms | Tot: 18s623ms | Loss: 0.175 | Acc: 94.314% (47157/50000) 391/391 ================>...............................................]  Step: 54ms | Tot: 5s260ms | Loss: 0.178 | Acc: 94.366% (13166/13952) 109/391 \n",
      " [================================================================>]  Step: 19ms | Tot: 1s912ms | Loss: 0.430 | Acc: 88.240% (8824/10000) 100/100 Step: 20ms | Tot: 1s326ms | Loss: 0.428 | Acc: 88.458% (6369/7200) 72/100   Step: 22ms | Tot: 1s402ms | Loss: 0.428 | Acc: 88.453% (6634/7500) 75/100 \n",
      "Test error is 0.1176 0.11760000000000004\n",
      "Hamming distance between rounds is 0.1194\n",
      "Test error of previous epoch is 0.141\n",
      "Saving..\n",
      "\n",
      "Epoch: 23\n",
      " [================================================================>]  Step: 46ms | Tot: 18s502ms | Loss: 0.162 | Acc: 94.544% (47272/50000) 391/391 ====>........................................................]  Step: 49ms | Tot: 2s498ms | Loss: 0.152 | Acc: 94.708% (6425/6784) 53/391 ..]  Step: 58ms | Tot: 2s557ms | Loss: 0.153 | Acc: 94.734% (6548/6912) 54/391 ]  Step: 56ms | Tot: 4s657ms | Loss: 0.160 | Acc: 94.733% (11762/12416) 97/391 \n",
      " [================================================================>]  Step: 17ms | Tot: 1s945ms | Loss: 0.479 | Acc: 87.240% (8724/10000) 100/100 100 \n",
      "Test error is 0.1276 0.12760000000000005\n",
      "Hamming distance between rounds is 0.1221\n",
      "Test error of previous epoch is 0.1176\n",
      "\n",
      "Epoch: 24\n",
      " [================================================================>]  Step: 42ms | Tot: 19s73ms | Loss: 0.156 | Acc: 94.846% (47423/50000) 391/391  =================>..............................................]  Step: 44ms | Tot: 5s457ms | Loss: 0.162 | Acc: 94.614% (13685/14464) 113/391 ====================>............................................]  Step: 49ms | Tot: 6s78ms | Loss: 0.162 | Acc: 94.700% (15152/16000) 125/391 \n",
      " [================================================================>]  Step: 16ms | Tot: 1s911ms | Loss: 0.464 | Acc: 87.740% (8774/10000) 100/100 \n",
      "Test error is 0.1226 0.12260000000000004\n",
      "Hamming distance between rounds is 0.1177\n",
      "Test error of previous epoch is 0.1276\n",
      "\n",
      "Epoch: 25\n",
      " [================================================================>]  Step: 47ms | Tot: 18s485ms | Loss: 0.145 | Acc: 95.170% (47585/50000) 391/391 ...]  Step: 56ms | Tot: 1s38ms | Loss: 0.150 | Acc: 95.109% (2800/2944) 23/391 ........]  Step: 42ms | Tot: 9s199ms | Loss: 0.147 | Acc: 95.071% (23608/24832) 194/391 =========================================================>......]  Step: 58ms | Tot: 16s735ms | Loss: 0.147 | Acc: 95.115% (42855/45056) 352/391 \n",
      " [================================================================>]  Step: 14ms | Tot: 1s922ms | Loss: 0.595 | Acc: 85.570% (8557/10000) 100/100 ============================>................................]  Step: 22ms | Tot: 968ms | Loss: 0.581 | Acc: 86.000% (4386/5100) 51/100 \n",
      "Test error is 0.1443 0.1443000000000001\n",
      "Hamming distance between rounds is 0.1204\n",
      "Test error of previous epoch is 0.1226\n",
      "\n",
      "Epoch: 26\n",
      " [================================================================>]  Step: 36ms | Tot: 18s650ms | Loss: 0.136 | Acc: 95.434% (47717/50000) 391/391 ==============>............................................]  Step: 62ms | Tot: 5s905ms | Loss: 0.141 | Acc: 95.331% (15131/15872) 124/391 ==========================>......................................]  Step: 51ms | Tot: 7s512ms | Loss: 0.140 | Acc: 95.337% (19281/20224) 158/391 ============================>................................]  Step: 51ms | Tot: 9s493ms | Loss: 0.138 | Acc: 95.352% (24288/25472) 199/391  326/391 \n",
      " [================================================================>]  Step: 18ms | Tot: 1s879ms | Loss: 0.471 | Acc: 87.900% (8790/10000) 100/100 ======>.....................................................]  Step: 17ms | Tot: 330ms | Loss: 0.451 | Acc: 87.947% (1671/1900) 19/100 \n",
      "Test error is 0.121 0.121\n",
      "Hamming distance between rounds is 0.1242\n",
      "Test error of previous epoch is 0.1443\n",
      "\n",
      "Epoch: 27\n",
      " [================================================================>]  Step: 45ms | Tot: 18s470ms | Loss: 0.128 | Acc: 95.766% (47883/50000) 391/391 ....]  Step: 58ms | Tot: 11s200ms | Loss: 0.129 | Acc: 95.711% (29280/30592) 239/391 ==============================================>..................]  Step: 47ms | Tot: 13s37ms | Loss: 0.131 | Acc: 95.641% (34033/35584) 278/391 \n",
      " [================================================================>]  Step: 17ms | Tot: 1s986ms | Loss: 0.396 | Acc: 89.420% (8942/10000) 100/100 Step: 19ms | Tot: 387ms | Loss: 0.343 | Acc: 90.105% (1712/1900) 19/100 ]  Step: 24ms | Tot: 1s178ms | Loss: 0.396 | Acc: 89.759% (5206/5800) 58/100   Step: 20ms | Tot: 1s748ms | Loss: 0.402 | Acc: 89.460% (7783/8700) 87/100 \n",
      "Test error is 0.1058 0.1058\n",
      "Hamming distance between rounds is 0.0909\n",
      "Test error of previous epoch is 0.121\n",
      "Saving..\n",
      "\n",
      "Epoch: 28\n",
      " [================================================================>]  Step: 40ms | Tot: 18s795ms | Loss: 0.120 | Acc: 96.064% (48032/50000) 391/391 ===>.........................................................]  Step: 49ms | Tot: 2s263ms | Loss: 0.108 | Acc: 96.452% (5926/6144) 48/391 ....]  Step: 48ms | Tot: 6s560ms | Loss: 0.115 | Acc: 96.219% (16873/17536) 137/391 \n",
      " [================================================================>]  Step: 14ms | Tot: 2s173ms | Loss: 0.404 | Acc: 89.180% (8918/10000) 100/100 .......................]  Step: 20ms | Tot: 311ms | Loss: 0.357 | Acc: 89.533% (1343/1500) 15/100 ...................................................]  Step: 25ms | Tot: 336ms | Loss: 0.365 | Acc: 89.500% (1432/1600) 16/100 ================>................................................]  Step: 18ms | Tot: 534ms | Loss: 0.401 | Acc: 88.962% (2313/2600) 26/100   Step: 25ms | Tot: 1s178ms | Loss: 0.404 | Acc: 89.125% (4991/5600) 56/100 .]  Step: 25ms | Tot: 1s203ms | Loss: 0.407 | Acc: 89.053% (5076/5700) 57/100 .]  Step: 24ms | Tot: 1s303ms | Loss: 0.407 | Acc: 88.967% (5427/6100) 61/100 \n",
      "Test error is 0.1082 0.10819999999999996\n",
      "Hamming distance between rounds is 0.0899\n",
      "Test error of previous epoch is 0.1058\n",
      "\n",
      "Epoch: 29\n",
      " [================================================================>]  Step: 41ms | Tot: 18s538ms | Loss: 0.118 | Acc: 96.072% (48036/50000) 391/391 ................................]  Step: 46ms | Tot: 984ms | Loss: 0.099 | Acc: 96.733% (2724/2816) 22/391 ....]  Step: 60ms | Tot: 7s934ms | Loss: 0.117 | Acc: 96.186% (20930/21760) 170/391 \n",
      " [================================================================>]  Step: 20ms | Tot: 2s157ms | Loss: 0.451 | Acc: 88.240% (8824/10000) 100/100  Step: 22ms | Tot: 1s501ms | Loss: 0.450 | Acc: 88.333% (6625/7500) 75/100 \n",
      "Test error is 0.1176 0.11760000000000004\n",
      "Hamming distance between rounds is 0.1033\n",
      "Test error of previous epoch is 0.1082\n",
      "\n",
      "Epoch: 30\n",
      " [================================================================>]  Step: 40ms | Tot: 18s868ms | Loss: 0.109 | Acc: 96.372% (48186/50000) 391/391 ........................]  Step: 60ms | Tot: 5s759ms | Loss: 0.105 | Acc: 96.539% (14952/15488) 121/391 ...]  Step: 62ms | Tot: 17s706ms | Loss: 0.110 | Acc: 96.347% (45260/46976) 367/391 ===============================================================>.]  Step: 62ms | Tot: 18s376ms | Loss: 0.110 | Acc: 96.363% (46871/48640) 380/391 \n",
      " [================================================================>]  Step: 18ms | Tot: 2s121ms | Loss: 0.418 | Acc: 89.070% (8907/10000) 100/100 ep: 23ms | Tot: 718ms | Loss: 0.385 | Acc: 89.545% (2955/3300) 33/100 \n",
      "Test error is 0.1093 0.10930000000000006\n",
      "Hamming distance between rounds is 0.1004\n",
      "Test error of previous epoch is 0.1176\n",
      "\n",
      "Epoch: 31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [================================================================>]  Step: 44ms | Tot: 19s1ms | Loss: 0.103 | Acc: 96.600% (48300/50000) 391/391 1 =====================>..........................................]  Step: 58ms | Tot: 6s263ms | Loss: 0.110 | Acc: 96.377% (16654/17280) 135/391 ==============================>..................................]  Step: 52ms | Tot: 8s850ms | Loss: 0.109 | Acc: 96.395% (23073/23936) 187/391 ..............]  Step: 65ms | Tot: 12s469ms | Loss: 0.105 | Acc: 96.513% (32243/33408) 261/391 ===========================================>.....................]  Step: 45ms | Tot: 12s560ms | Loss: 0.105 | Acc: 96.513% (32490/33664) 263/391 ==========================================>...................]  Step: 48ms | Tot: 13s81ms | Loss: 0.105 | Acc: 96.532% (33732/34944) 273/391 ==============================================>...............]  Step: 48ms | Tot: 14s289ms | Loss: 0.104 | Acc: 96.549% (36704/38016) 297/391 \n",
      " [================================================================>]  Step: 16ms | Tot: 1s914ms | Loss: 0.418 | Acc: 89.460% (8946/10000) 100/100 \n",
      "Test error is 0.1054 0.10540000000000005\n",
      "Hamming distance between rounds is 0.0939\n",
      "Test error of previous epoch is 0.1093\n",
      "Saving..\n",
      "\n",
      "Epoch: 32\n",
      " [================================================================>]  Step: 39ms | Tot: 18s866ms | Loss: 0.098 | Acc: 96.780% (48390/50000) 391/391 =====>...........................................]  Step: 44ms | Tot: 6s328ms | Loss: 0.098 | Acc: 96.780% (16228/16768) 131/391 ========================>........................................]  Step: 44ms | Tot: 7s257ms | Loss: 0.099 | Acc: 96.771% (18580/19200) 150/391   Step: 43ms | Tot: 9s168ms | Loss: 0.100 | Acc: 96.747% (23405/24192) 189/391 ========>...............]  Step: 51ms | Tot: 14s422ms | Loss: 0.099 | Acc: 96.780% (36792/38016) 297/391 =>..]  Step: 57ms | Tot: 18s177ms | Loss: 0.099 | Acc: 96.761% (46569/48128) 376/391 \n",
      " [================================================================>]  Step: 24ms | Tot: 2s71ms | Loss: 0.492 | Acc: 88.030% (8803/10000) 100/100 61/100 \n",
      "Test error is 0.1197 0.11970000000000003\n",
      "Hamming distance between rounds is 0.1027\n",
      "Test error of previous epoch is 0.1054\n",
      "\n",
      "Epoch: 33\n",
      " [================================================================>]  Step: 46ms | Tot: 18s655ms | Loss: 0.093 | Acc: 96.930% (48465/50000) 391/391 tep: 65ms | Tot: 813ms | Loss: 0.107 | Acc: 96.369% (2097/2176) 17/391 ................................................]  Step: 54ms | Tot: 3s462ms | Loss: 0.094 | Acc: 96.970% (9185/9472) 74/391 \n",
      " [================================================================>]  Step: 20ms | Tot: 2s3ms | Loss: 0.473 | Acc: 88.150% (8815/10000) 100/100 \n",
      "Test error is 0.1185 0.11849999999999994\n",
      "Hamming distance between rounds is 0.11\n",
      "Test error of previous epoch is 0.1197\n",
      "\n",
      "Epoch: 34\n",
      " [================================================================>]  Step: 38ms | Tot: 18s909ms | Loss: 0.089 | Acc: 97.062% (48531/50000) 391/391 391 =========================================================>.......]  Step: 48ms | Tot: 16s735ms | Loss: 0.090 | Acc: 97.023% (43218/44544) 348/391 \n",
      " [================================================================>]  Step: 15ms | Tot: 1s904ms | Loss: 0.500 | Acc: 88.540% (8854/10000) 100/100   Step: 22ms | Tot: 45ms | Loss: 0.460 | Acc: 86.333% (259/300) 3/100 .]  Step: 18ms | Tot: 456ms | Loss: 0.486 | Acc: 88.120% (2203/2500) 25/100 ............]  Step: 21ms | Tot: 1s58ms | Loss: 0.501 | Acc: 88.727% (4880/5500) 55/100 \n",
      "Test error is 0.1146 0.11459999999999992\n",
      "Hamming distance between rounds is 0.0957\n",
      "Test error of previous epoch is 0.1185\n",
      "\n",
      "Epoch: 35\n",
      " [================================================================>]  Step: 52ms | Tot: 18s996ms | Loss: 0.091 | Acc: 96.992% (48496/50000) 391/391  Step: 54ms | Tot: 5s422ms | Loss: 0.084 | Acc: 97.179% (14056/14464) 113/391 ======================>..........................................]  Step: 45ms | Tot: 6s433ms | Loss: 0.087 | Acc: 97.153% (16788/17280) 135/391  212/391 =>..............]  Step: 50ms | Tot: 14s354ms | Loss: 0.092 | Acc: 96.945% (37475/38656) 302/391 \n",
      " [================================================================>]  Step: 16ms | Tot: 1s959ms | Loss: 0.451 | Acc: 88.850% (8885/10000) 100/100 \n",
      "Test error is 0.1115 0.11150000000000004\n",
      "Hamming distance between rounds is 0.0884\n",
      "Test error of previous epoch is 0.1146\n",
      "\n",
      "Epoch: 36\n",
      " [================================================================>]  Step: 40ms | Tot: 18s653ms | Loss: 0.076 | Acc: 97.526% (48763/50000) 391/391 ...........]  Step: 53ms | Tot: 2s464ms | Loss: 0.069 | Acc: 97.611% (6497/6656) 52/391   Step: 60ms | Tot: 8s746ms | Loss: 0.081 | Acc: 97.390% (23062/23680) 185/391 \n",
      " [================================================================>]  Step: 20ms | Tot: 2s70ms | Loss: 0.442 | Acc: 89.210% (8921/10000) 100/100 tep: 24ms | Tot: 552ms | Loss: 0.430 | Acc: 88.966% (2580/2900) 29/100 \n",
      "Test error is 0.1079 0.1079000000000001\n",
      "Hamming distance between rounds is 0.0879\n",
      "Test error of previous epoch is 0.1115\n",
      "\n",
      "Epoch: 37\n",
      " [================================================================>]  Step: 43ms | Tot: 18s878ms | Loss: 0.079 | Acc: 97.430% (48715/50000) 391/391 .....]  Step: 47ms | Tot: 7s238ms | Loss: 0.080 | Acc: 97.357% (18568/19072) 149/391 ========================================================>........]  Step: 47ms | Tot: 16s398ms | Loss: 0.080 | Acc: 97.388% (42134/43264) 338/391 \n",
      " [================================================================>]  Step: 17ms | Tot: 1s946ms | Loss: 0.503 | Acc: 88.660% (8866/10000) 100/100  Step: 21ms | Tot: 1s494ms | Loss: 0.502 | Acc: 88.628% (6913/7800) 78/100 \n",
      "Test error is 0.1134 0.11340000000000006\n",
      "Hamming distance between rounds is 0.1011\n",
      "Test error of previous epoch is 0.1079\n",
      "\n",
      "Epoch: 38\n",
      " [================================================================>]  Step: 44ms | Tot: 18s782ms | Loss: 0.077 | Acc: 97.450% (48725/50000) 391/391 ==========>...................................................]  Step: 40ms | Tot: 3s874ms | Loss: 0.081 | Acc: 97.252% (10332/10624) 83/391 ............]  Step: 55ms | Tot: 4s200ms | Loss: 0.082 | Acc: 97.248% (11203/11520) 90/391 .]  Step: 59ms | Tot: 7s308ms | Loss: 0.078 | Acc: 97.438% (19207/19712) 154/391  205/391 =======================================>.........................]  Step: 50ms | Tot: 11s633ms | Loss: 0.077 | Acc: 97.433% (30056/30848) 241/391 ]  Step: 58ms | Tot: 12s83ms | Loss: 0.077 | Acc: 97.441% (31181/32000) 250/391 \n",
      " [================================================================>]  Step: 18ms | Tot: 2s20ms | Loss: 0.482 | Acc: 89.070% (8907/10000) 100/100 ..............................]  Step: 21ms | Tot: 724ms | Loss: 0.456 | Acc: 89.167% (3210/3600) 36/100 .......................]  Step: 19ms | Tot: 1s249ms | Loss: 0.479 | Acc: 89.048% (5521/6200) 62/100 \n",
      "Test error is 0.1093 0.10930000000000006\n",
      "Hamming distance between rounds is 0.1019\n",
      "Test error of previous epoch is 0.1134\n",
      "\n",
      "Epoch: 39\n",
      " [================================================================>]  Step: 55ms | Tot: 18s675ms | Loss: 0.071 | Acc: 97.632% (48816/50000) 391/391 ................]  Step: 47ms | Tot: 1s481ms | Loss: 0.068 | Acc: 97.552% (3746/3840) 30/391 ....]  Step: 56ms | Tot: 10s366ms | Loss: 0.075 | Acc: 97.427% (27186/27904) 218/391 ======================================>..........................]  Step: 50ms | Tot: 11s134ms | Loss: 0.073 | Acc: 97.515% (29083/29824) 233/391 ==================================================>.............]  Step: 53ms | Tot: 14s782ms | Loss: 0.070 | Acc: 97.634% (38866/39808) 311/391 \n",
      " [================================================================>]  Step: 21ms | Tot: 2s72ms | Loss: 0.428 | Acc: 89.670% (8967/10000) 100/100 ...........................................]  Step: 25ms | Tot: 534ms | Loss: 0.415 | Acc: 89.889% (2427/2700) 27/100 ========================>.......]  Step: 21ms | Tot: 1s842ms | Loss: 0.431 | Acc: 89.640% (7978/8900) 89/100 \n",
      "Test error is 0.1033 0.10329999999999995\n",
      "Hamming distance between rounds is 0.0949\n",
      "Test error of previous epoch is 0.1093\n",
      "Saving..\n",
      "\n",
      "Epoch: 40\n",
      " [================================================================>]  Step: 47ms | Tot: 18s924ms | Loss: 0.069 | Acc: 97.684% (48842/50000) 391/391 ...]  Step: 56ms | Tot: 10s126ms | Loss: 0.068 | Acc: 97.720% (26142/26752) 209/391 =============================================>...................]  Step: 50ms | Tot: 13s484ms | Loss: 0.070 | Acc: 97.639% (34619/35456) 277/391 \n",
      " [================================================================>]  Step: 24ms | Tot: 2s52ms | Loss: 0.471 | Acc: 88.980% (8898/10000) 100/100 57/100 \n",
      "Test error is 0.1102 0.11019999999999996\n",
      "Hamming distance between rounds is 0.1014\n",
      "Test error of previous epoch is 0.1033\n",
      "\n",
      "Epoch: 41\n",
      " [================================================================>]  Step: 50ms | Tot: 19s215ms | Loss: 0.064 | Acc: 97.850% (48925/50000) 391/391 =====================>..........................................]  Step: 62ms | Tot: 6s545ms | Loss: 0.068 | Acc: 97.795% (16899/17280) 135/391 ...................................]  Step: 51ms | Tot: 7s116ms | Loss: 0.067 | Acc: 97.789% (18400/18816) 147/391  242/391 ============>.........]  Step: 60ms | Tot: 16s277ms | Loss: 0.064 | Acc: 97.837% (41702/42624) 333/391 ========================================================>........]  Step: 52ms | Tot: 16s709ms | Loss: 0.064 | Acc: 97.853% (42836/43776) 342/391 \n",
      " [================================================================>]  Step: 18ms | Tot: 2s12ms | Loss: 0.433 | Acc: 89.800% (8980/10000) 100/100 90/100 \n",
      "Test error is 0.102 0.10199999999999998\n",
      "Hamming distance between rounds is 0.0956\n",
      "Test error of previous epoch is 0.1102\n",
      "Saving..\n",
      "\n",
      "Epoch: 42\n",
      " [================================================================>]  Step: 49ms | Tot: 18s801ms | Loss: 0.065 | Acc: 97.878% (48939/50000) 391/391 =======>.....................................................]  Step: 47ms | Tot: 3s443ms | Loss: 0.066 | Acc: 97.817% (9140/9344) 73/391  79/391 ]  Step: 45ms | Tot: 13s811ms | Loss: 0.065 | Acc: 97.856% (36324/37120) 290/391 ..]  Step: 60ms | Tot: 14s867ms | Loss: 0.064 | Acc: 97.869% (39085/39936) 312/391   Step: 57ms | Tot: 15s546ms | Loss: 0.065 | Acc: 97.885% (40720/41600) 325/391 ========>]  Step: 53ms | Tot: 18s557ms | Loss: 0.065 | Acc: 97.885% (48363/49408) 386/391 \n",
      " [================================================================>]  Step: 18ms | Tot: 2s15ms | Loss: 0.438 | Acc: 89.610% (8961/10000) 100/100 =========================================================>......]  Step: 22ms | Tot: 1s829ms | Loss: 0.442 | Acc: 89.527% (8147/9100) 91/100 \n",
      "Test error is 0.1039 0.10389999999999999\n",
      "Hamming distance between rounds is 0.09\n",
      "Test error of previous epoch is 0.102\n",
      "\n",
      "Epoch: 43\n",
      " [================================================================>]  Step: 42ms | Tot: 18s736ms | Loss: 0.062 | Acc: 97.982% (48991/50000) 391/391  Step: 59ms | Tot: 4s632ms | Loss: 0.061 | Acc: 97.962% (12163/12416) 97/391 =================================================>...............]  Step: 42ms | Tot: 14s136ms | Loss: 0.062 | Acc: 97.991% (37127/37888) 296/391   Step: 48ms | Tot: 16s280ms | Loss: 0.061 | Acc: 97.996% (42648/43520) 340/391 \n",
      " [================================================================>]  Step: 18ms | Tot: 1s966ms | Loss: 0.478 | Acc: 89.350% (8935/10000) 100/100 ...........................]  Step: 18ms | Tot: 39ms | Loss: 0.379 | Acc: 90.000% (270/300) 3/100  19/100 ===>............]  Step: 20ms | Tot: 1s630ms | Loss: 0.471 | Acc: 89.463% (7336/8200) 82/100  83/100 \n",
      "Test error is 0.1065 0.10650000000000004\n",
      "Hamming distance between rounds is 0.0964\n",
      "Test error of previous epoch is 0.1039\n",
      "\n",
      "Epoch: 44\n",
      " [================================================================>]  Step: 45ms | Tot: 19s13ms | Loss: 0.061 | Acc: 97.954% (48977/50000) 391/391  ........................]  Step: 57ms | Tot: 4s578ms | Loss: 0.056 | Acc: 98.106% (11553/11776) 92/391   Step: 56ms | Tot: 5s856ms | Loss: 0.057 | Acc: 98.017% (14930/15232) 119/391 ===================================================>............]  Step: 50ms | Tot: 15s457ms | Loss: 0.061 | Acc: 97.953% (39620/40448) 316/391 \n",
      " [================================================================>]  Step: 17ms | Tot: 1s945ms | Loss: 0.439 | Acc: 89.670% (8967/10000) 100/100 ===============>.............................................]  Step: 24ms | Tot: 629ms | Loss: 0.405 | Acc: 90.161% (2795/3100) 31/100 \n",
      "Test error is 0.1033 0.10329999999999995\n",
      "Hamming distance between rounds is 0.0945\n",
      "Test error of previous epoch is 0.1065\n",
      "\n",
      "Epoch: 45\n",
      " [================================================================>]  Step: 47ms | Tot: 18s778ms | Loss: 0.057 | Acc: 98.134% (49067/50000) 391/391 =====================>..........................................]  Step: 56ms | Tot: 6s594ms | Loss: 0.058 | Acc: 97.973% (17306/17664) 138/391 .......................]  Step: 58ms | Tot: 8s274ms | Loss: 0.057 | Acc: 98.070% (21591/22016) 172/391 ===================>.........................]  Step: 52ms | Tot: 11s374ms | Loss: 0.056 | Acc: 98.130% (29643/30208) 236/391 =====================================================>...........]  Step: 56ms | Tot: 15s441ms | Loss: 0.057 | Acc: 98.114% (40313/41088) 321/391 \n",
      " [================================================================>]  Step: 21ms | Tot: 2s81ms | Loss: 0.448 | Acc: 89.560% (8956/10000) 100/100 \n",
      "Test error is 0.1044 0.10439999999999994\n",
      "Hamming distance between rounds is 0.0929\n",
      "Test error of previous epoch is 0.1033\n",
      "\n",
      "Epoch: 46\n",
      " [================================================================>]  Step: 44ms | Tot: 18s775ms | Loss: 0.055 | Acc: 98.138% (49069/50000) 391/391 ...........................................................]  Step: 51ms | Tot: 742ms | Loss: 0.060 | Acc: 97.518% (2122/2176) 17/391  73/391 =======================================================>.........]  Step: 50ms | Tot: 15s952ms | Loss: 0.054 | Acc: 98.169% (41718/42496) 332/391 ============================================================>....]  Step: 51ms | Tot: 17s418ms | Loss: 0.054 | Acc: 98.173% (45615/46464) 363/391 \n",
      " [================================================================>]  Step: 19ms | Tot: 2s64ms | Loss: 0.463 | Acc: 89.410% (8941/10000) 100/100 ===>..........................................]  Step: 23ms | Tot: 701ms | Loss: 0.440 | Acc: 89.600% (3136/3500) 35/100 =================================================>...............]  Step: 23ms | Tot: 1s570ms | Loss: 0.458 | Acc: 89.506% (6892/7700) 77/100  90/100 \n",
      "Test error is 0.1059 0.1059\n",
      "Hamming distance between rounds is 0.0931\n",
      "Test error of previous epoch is 0.1044\n",
      "\n",
      "Epoch: 47\n",
      " [================================================================>]  Step: 38ms | Tot: 18s735ms | Loss: 0.057 | Acc: 98.126% (49063/50000) 391/391 ========================>.......................................]  Step: 40ms | Tot: 7s275ms | Loss: 0.060 | Acc: 97.995% (19066/19456) 152/391 =================>..............]  Step: 48ms | Tot: 14s803ms | Loss: 0.057 | Acc: 98.112% (38554/39296) 307/391 .]  Step: 62ms | Tot: 17s416ms | Loss: 0.057 | Acc: 98.120% (45716/46592) 364/391 \n",
      " [================================================================>]  Step: 20ms | Tot: 2s37ms | Loss: 0.445 | Acc: 89.830% (8983/10000) 100/100 \n",
      "Test error is 0.1017 0.10170000000000001\n",
      "Hamming distance between rounds is 0.0819\n",
      "Test error of previous epoch is 0.1059\n",
      "Saving..\n",
      "\n",
      "Epoch: 48\n",
      " [================================================================>]  Step: 45ms | Tot: 19s49ms | Loss: 0.053 | Acc: 98.264% (49132/50000) 391/391  ...]  Step: 62ms | Tot: 13s606ms | Loss: 0.054 | Acc: 98.228% (35205/35840) 280/391 .....]  Step: 60ms | Tot: 15s985ms | Loss: 0.056 | Acc: 98.186% (41348/42112) 329/391 ..]  Step: 56ms | Tot: 16s503ms | Loss: 0.055 | Acc: 98.205% (42739/43520) 340/391  385/391 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [================================================================>]  Step: 24ms | Tot: 2s94ms | Loss: 0.464 | Acc: 89.990% (8999/10000) 100/100 .....................]  Step: 26ms | Tot: 497ms | Loss: 0.474 | Acc: 89.417% (2146/2400) 24/100 .....]  Step: 19ms | Tot: 1s691ms | Loss: 0.460 | Acc: 89.976% (7378/8200) 82/100 ===================================>...........]  Step: 22ms | Tot: 1s734ms | Loss: 0.462 | Acc: 89.940% (7555/8400) 84/100 \n",
      "Test error is 0.1001 0.10010000000000008\n",
      "Hamming distance between rounds is 0.0864\n",
      "Test error of previous epoch is 0.1017\n",
      "Saving..\n",
      "\n",
      "Epoch: 49\n",
      " [================================================================>]  Step: 46ms | Tot: 18s962ms | Loss: 0.048 | Acc: 98.420% (49210/50000) 391/391 .........]  Step: 45ms | Tot: 4s141ms | Loss: 0.049 | Acc: 98.261% (10565/10752) 84/391  171/391 ..]  Step: 57ms | Tot: 9s687ms | Loss: 0.053 | Acc: 98.266% (25156/25600) 200/391 ============================================================>....]  Step: 54ms | Tot: 17s720ms | Loss: 0.049 | Acc: 98.388% (45967/46720) 365/391 \n",
      " [================================================================>]  Step: 21ms | Tot: 1s883ms | Loss: 0.465 | Acc: 90.060% (9006/10000) 100/100 ..............]  Step: 22ms | Tot: 222ms | Loss: 0.409 | Acc: 90.667% (1088/1200) 12/100 ===========================================>.....................]  Step: 23ms | Tot: 1s291ms | Loss: 0.449 | Acc: 90.132% (6129/6800) 68/100 \n",
      "Test error is 0.0994 0.09939999999999993\n",
      "Hamming distance between rounds is 0.0793\n",
      "Test error of previous epoch is 0.1001\n",
      "Saving..\n"
     ]
    }
   ],
   "source": [
    "%run normal_main.py --task=CIFAR10 --lr=0.001 --epochs=50 --algorithm=Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def imshow(img):\n",
    "    plt.imshow(np.transpose(img.numpy(), (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from utils.data_processing.get_dataset import get_train_data, get_validation_data\n",
    "testset = get_validation_data('CIFAR10')\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "data,labels=iter(testloader).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.min(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs=global_net(adversarial_images)\n",
    "probs=torch.nn.functional.softmax(probs,dim=1)\n",
    "probs_max,pred = probs.max(1)\n",
    "probs_max=probs_max.cpu()\n",
    "pred=pred.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 0.9973, 1.0000, 1.0000, 1.0000, 0.9999, 0.9999, 1.0000, 1.0000,\n",
       "        0.9999, 1.0000, 1.0000, 0.9995, 0.9999, 1.0000, 1.0000, 1.0000, 0.9993,\n",
       "        1.0000, 0.9999, 1.0000, 0.9184, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 0.9999, 1.0000, 1.0000, 1.0000, 0.9999, 1.0000, 0.9998, 1.0000,\n",
       "        0.9999, 0.9883, 0.9873, 1.0000, 0.9999, 1.0000, 1.0000, 1.0000, 0.9992,\n",
       "        0.6792, 1.0000, 1.0000, 1.0000, 1.0000, 0.9999, 1.0000, 0.9999, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 0.9998, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        0.9998, 1.0000, 0.9999, 1.0000, 1.0000, 1.0000, 0.9999, 0.9993, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 0.9999, 1.0000, 1.0000, 0.9999, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        0.9999, 1.0000, 0.9995, 0.9999, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        0.9999], grad_fn=<CopyBackwards>)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False, False, False,  True, False,  True, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False,  True, False,  True,  True, False, False, False,  True, False,\n",
       "         True, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False,  True, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,  True,\n",
       "        False, False, False,  True, False, False, False, False,  True, False])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred != labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(pred == labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "atk = torchattacks.PGD(global_net, eps=8/255, alpha=2/255, steps=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "adversarial_images = atk(data, labels).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "imshow(adversarial_images[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "imshow(data[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0314)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(adversarial_images[3] - data[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03137254901960784"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "782"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "391*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "742.9"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "782*0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
